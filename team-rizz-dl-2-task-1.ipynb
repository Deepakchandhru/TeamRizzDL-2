{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3531703,"sourceType":"datasetVersion","datasetId":2123951}],"dockerImageVersionId":30761,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/deepakchandhru/team-rizz-dl-2-task-1?scriptVersionId=195806667\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize\nfrom gensim.models import Word2Vec\nfrom scipy import spatial\nimport networkx as nx\nimport csv\n\ndf = pd.read_csv(\"/kaggle/input/medium-articles/medium_articles.csv\")\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2024-09-08T08:44:16.715626Z","iopub.execute_input":"2024-09-08T08:44:16.716312Z","iopub.status.idle":"2024-09-08T08:44:35.000907Z","shell.execute_reply.started":"2024-09-08T08:44:16.716262Z","shell.execute_reply":"2024-09-08T08:44:34.999546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[['title','text']]\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-08T08:45:15.185708Z","iopub.execute_input":"2024-09-08T08:45:15.186179Z","iopub.status.idle":"2024-09-08T08:45:15.24348Z","shell.execute_reply.started":"2024-09-08T08:45:15.186126Z","shell.execute_reply":"2024-09-08T08:45:15.242163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop_duplicates()\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2024-09-08T08:45:18.770361Z","iopub.execute_input":"2024-09-08T08:45:18.770776Z","iopub.status.idle":"2024-09-08T08:45:23.428863Z","shell.execute_reply.started":"2024-09-08T08:45:18.770737Z","shell.execute_reply":"2024-09-08T08:45:23.427418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['text'] = df['text'].str.replace('\\n','<|n|>')","metadata":{"execution":{"iopub.status.busy":"2024-09-08T08:45:25.410408Z","iopub.execute_input":"2024-09-08T08:45:25.410831Z","iopub.status.idle":"2024-09-08T08:45:28.814806Z","shell.execute_reply.started":"2024-09-08T08:45:25.410793Z","shell.execute_reply":"2024-09-08T08:45:28.813625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_blog = df['text'][10]\nprint(sample_blog)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T08:45:32.035237Z","iopub.execute_input":"2024-09-08T08:45:32.035674Z","iopub.status.idle":"2024-09-08T08:45:32.05124Z","shell.execute_reply.started":"2024-09-08T08:45:32.035633Z","shell.execute_reply":"2024-09-08T08:45:32.049886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = sent_tokenize(sample_blog)\nsentences_clean = [re.sub(r'[^\\w\\s]', '', sentence.lower()) for sentence in sentences]\nstop_words = stopwords.words('english')\nsentence_tokens = [[words for words in sentence.split(' ') if words not in stop_words] for sentence in sentences_clean]\nw2v = Word2Vec(sentence_tokens, vector_size=1, min_count=1, epochs=1000)\nsentence_embeddings = [[w2v.wv.get_vector(word)[0] for word in words] for words in sentence_tokens]\nmax_len = max([len(tokens) for tokens in sentence_tokens])\nsentence_embeddings = [np.pad(embedding, (0, max_len - len(embedding)), 'constant') for embedding in sentence_embeddings]\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T08:45:36.906132Z","iopub.execute_input":"2024-09-08T08:45:36.907111Z","iopub.status.idle":"2024-09-08T08:45:39.554907Z","shell.execute_reply.started":"2024-09-08T08:45:36.907033Z","shell.execute_reply":"2024-09-08T08:45:39.553458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity_matrix = np.zeros([len(sentence_tokens), len(sentence_tokens)])\nfor i, row_embedding in enumerate(sentence_embeddings):\n    for j, column_embedding in enumerate(sentence_embeddings):\n        similarity_matrix[i][j] = 1 - spatial.distance.cosine(row_embedding, column_embedding)\nnx_graph = nx.from_numpy_array(similarity_matrix)\nscores = nx.pagerank(nx_graph, max_iter=600)\ntop_sentence = {sentence: scores[index] for index, sentence in enumerate(sentences)}\nsentNeeded = round(0.25 * len(sentences)) - 1\ntop_dict = dict(sorted(top_sentence.items(), key=lambda x: x[1], reverse=True)[:sentNeeded])\nsummary = \"\"\nfor sent in sentences:\n    if sent in top_dict.keys():\n        summary += sent\nprint(summary)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T08:49:08.701395Z","iopub.execute_input":"2024-09-08T08:49:08.701867Z","iopub.status.idle":"2024-09-08T08:49:08.787563Z","shell.execute_reply.started":"2024-09-08T08:49:08.701824Z","shell.execute_reply":"2024-09-08T08:49:08.786167Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = 0\n\ndef generateSummary(blog):\n    global count\n    count+=1\n    print(\"Summarising Blog\",count)\n    \n    try:\n        sentences = sent_tokenize(sample_blog)\n        sentences_clean = [re.sub(r'[^\\w\\s]', '', sentence.lower()) for sentence in sentences]\n        stop_words = stopwords.words('english')\n        sentence_tokens = [[words for words in sentence.split(' ') if words not in stop_words] for sentence in sentences_clean]\n        w2v = Word2Vec(sentence_tokens, vector_size=1, min_count=1, epochs=1000)\n        sentence_embeddings = [[w2v.wv.get_vector(word)[0] for word in words] for words in sentence_tokens]\n        max_len = max([len(tokens) for tokens in sentence_tokens])\n        sentence_embeddings = [np.pad(embedding, (0, max_len - len(embedding)), 'constant') for embedding in sentence_embeddings]\n        similarity_matrix = np.zeros([len(sentence_tokens), len(sentence_tokens)])\n        for i, row_embedding in enumerate(sentence_embeddings):\n            for j, column_embedding in enumerate(sentence_embeddings):\n                similarity_matrix[i][j] = 1 - spatial.distance.cosine(row_embedding, column_embedding)\n        nx_graph = nx.from_numpy_array(similarity_matrix)\n        scores = nx.pagerank(nx_graph, max_iter=600)\n        top_sentence = {sentence: scores[index] for index, sentence in enumerate(sentences)}\n        sentNeeded = round(0.25 * len(sentences)) - 1\n        top_dict = dict(sorted(top_sentence.items(), key=lambda x: x[1], reverse=True)[:sentNeeded])\n        summary = \"\"\n        for sent in sentences:\n            if sent in top_dict.keys():\n                summary += sent\n        print(summary)\n    except:\n        return float(\"NaN\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-08T08:55:47.510628Z","iopub.execute_input":"2024-09-08T08:55:47.511118Z","iopub.status.idle":"2024-09-08T08:55:47.526788Z","shell.execute_reply.started":"2024-09-08T08:55:47.511054Z","shell.execute_reply":"2024-09-08T08:55:47.525628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nfilename = 'articlesSet.csv'\nfields = ['title','summary','content']\n\nwith open(filename,'a') as csvfile:\n    csvwriter = csv.writer(csvfile)\n    csvwriter.writerow(fields)\n    \n    def callback(row):\n        summary = generateSummary(row['text'])\n        if(type(summary)!=str):\n            return\n        rows = [row['title'],summary,row['text']]\n        csvwriter.writerow(rows)\n        \n    df.apply(callback,axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-09-08T09:14:39.581371Z","iopub.execute_input":"2024-09-08T09:14:39.581829Z"},"trusted":true},"execution_count":null,"outputs":[]}]}